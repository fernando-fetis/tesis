{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, hidden_size)\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]  # (B, N, hidden_size)\n",
    "        x = x + self.mlp(self.norm2(x))  # (B, N, hidden_size)\n",
    "        return x  # (B, N, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiT(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_blocks, patch_size, in_channels=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_embed = nn.Linear(in_channels * patch_size * patch_size, hidden_size)\n",
    "        self.blocks = nn.ModuleList([DiTBlock(hidden_size, num_heads) for _ in range(num_blocks)])\n",
    "        self.final_norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, in_channels * patch_size * patch_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: (B, C, H, W), t: (B,)\n",
    "        B, C, H, W = x.shape\n",
    "        assert C == self.in_channels, f\"Expected {self.in_channels} channels, got {C}\"\n",
    "\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).reshape(B, -1, C * self.patch_size * self.patch_size)  # (B, N, C * patch_size * patch_size), where N = (H * W) / (patch_size * patch_size)\n",
    "\n",
    "        x = self.patch_embed(x)  # (B, N, hidden_size)\n",
    "\n",
    "        t = t.unsqueeze(1).expand(-1, x.size(1))  # (B, N)\n",
    "        x = x + t.unsqueeze(-1)  # (B, N, hidden_size)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # (B, N, hidden_size)\n",
    "\n",
    "        x = self.final_norm(x)  # (B, N, hidden_size)\n",
    "\n",
    "        x = self.head(x)  # (B, N, C * patch_size * patch_size)\n",
    "\n",
    "        x = x.reshape(B, H // self.patch_size, W // self.patch_size, C, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 3, 1, 4, 2, 5).reshape(B, C, H, W)\n",
    "\n",
    "        return x  # (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo:\n",
    "hidden_size = 48\n",
    "num_heads = 3\n",
    "num_blocks = 2\n",
    "patch_size = 2\n",
    "in_channels = 4\n",
    "\n",
    "model = DiT(hidden_size, num_heads, num_blocks, patch_size, in_channels)\n",
    "x = torch.randn(1, in_channels, 32, 32)  # (1, 4, 32, 32)\n",
    "t = torch.randint(0, 1000, (1,))  # (1,)\n",
    "output = model(x, t)\n",
    "\n",
    "assert output.shape == torch.Size([1, 4, 32, 32])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
