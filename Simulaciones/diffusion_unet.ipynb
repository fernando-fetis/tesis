{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Arquitectura U-Net para modelos de difusión</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloques de la U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tiempo $t$ será codificado en un vector de dimensión $d$ dado por la concatenación de dos vectores $\\sin(u(t)),\\,\\cos(u(t))\\in\\mathbb{R}^\\frac{d}{2}$, donde las operaciones $\\sin$ y $\\cos$ son aplicadas elemento a elemento y $u(t)\\in\\mathbb{R}^\\frac{d}{2}$ es el vector definido por:\n",
    "\n",
    "$$\n",
    "u(t)_i = \\frac{t}{10000^{\\frac{i}{\\frac{d}{2} - 1}}}\n",
    "$$\n",
    "\n",
    "Posterior a esto, dicho vector será pasado a través de 2 capas fully connected para entregar una codificación temporal con el número de canales (dimensiones) pedida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - n_channels: número de dimensiones del embedding.\n",
    "        '''\n",
    "\n",
    "        assert n_channels % 8 == 0, 'la cantidad de canales debe ser divisible por 8.'\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.fc1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        self.fc2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - t[batch_size]: batch de posiciones temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - embedding[batch_size, n_channels]: embedding del batch temporal.\n",
    "        '''\n",
    "\n",
    "        d = self.n_channels // 4\n",
    "        i = torch.arange(d/2)\n",
    "        u_t = 10_000 ** (- i / (d/2 - 1))\n",
    "        u_t = t[:, None] * u_t[None,  :]\n",
    "\n",
    "        embedding = torch.cat((u_t.sin(), u_t.cos()), dim=1)\n",
    "        embedding = self.activation(self.fc1(embedding))\n",
    "        embedding = self.fc2(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "batch_size, n_channels = 128, 32\n",
    "\n",
    "time_embedding = TimeEmbedding(n_channels)\n",
    "t = torch.randint(0, 10000, size=[batch_size])\n",
    "embedding = time_embedding(t)\n",
    "\n",
    "assert embedding.shape == torch.Size([batch_size, n_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloque residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada bloque residual está compuesto por dos convoluciones (que preservan la resolución) seguidas de una conexión residual con el input del bloque. Las convoluciones seguirán el orden `normalización -> activación -> convolución`, y para la segunda convolución se agregará `dropout` entre la activación y la convolución.\n",
    "\n",
    "Por otra parte, el embedding temporal será sumado entre medio de las dos convoluciones. Para esto, se proyectará el embedding temporal para que la cantidad de canales del embedding temporal coincida con la cantidad de canales de la convolución y así poder realizar la suma.\n",
    "\n",
    "Dado que la cantidad de canales en la entrada y la salida del bloque no necesariamente deben coincidir, al final del forward se proyectarán los canales de la entrada para poder realizar la conexión residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32, dropout: float = 0.1):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - in_channels: cantidad de canales en la entrada.\n",
    "            - out_channels: cantidad de canales en la salida.\n",
    "            - time_channels: cantidad de canales para el embedding temporal.\n",
    "            - n_groups: cantidad de grupos para group normalization.\n",
    "            - dropout: tasa de dropout.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert in_channels % n_groups == out_channels % n_groups == 0, 'la cantidad de canales debe ser divisible por n_groups.'\n",
    "\n",
    "        # Convolución 1:\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        # Convolución 2:\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        # Proyector de canales para la conexión residual:\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_projector = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.residual_projector = nn.Identity()\n",
    "\n",
    "        # Proyector de canales para el embedding temporal:\n",
    "        self.time_projector = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor, time_embedding: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, in_channels, height, width]: batch de entrada.\n",
    "            - time_embedding[batch_size, time_channels]: batch de embedding temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, out_channels, height, width]: batch de salida.\n",
    "        '''\n",
    "\n",
    "        output = self.conv1(self.activation(self.norm1(input)))\n",
    "        output += self.time_projector(self.activation(time_embedding))[:, :, None, None]\n",
    "        output = self.conv2(self.dropout(self.activation(self.norm2(output))))\n",
    "        output += self.residual_projector(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "in_channels, out_channels, time_channels = 32, 64, 5\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "residual_block = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "input = torch.randn([batch_size, in_channels, height, width])\n",
    "time_embedding = torch.randn([batch_size, time_channels])\n",
    "output = residual_block(input, time_embedding)\n",
    "\n",
    "assert output.shape == torch.Size([batch_size, out_channels, height, width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloque de self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se implementará un módulo multicabezal de self-attention cuya estructura será similar a la que se propone en el paper de Transformers. Este módulo será aplicado luego de algunos bloques residuales.\n",
    "\n",
    "Dado que en este caso se trabajará con imágenes (tensores de rango 3) en vez de secuencias (tensores de rango 2), las dimensiones de alto y ancho serán aplanadas en un vector para así representar una secuencia de largo igual a la cantidad pixeles de la imagen. Con esto, cada tiempo de la secuencia estará representado por un vector de largo igual a la cantidad de canales de la imagen.\n",
    "\n",
    "Dicho de otra forma una, una secuencia genérica se representa por una matriz $X\\in\\mathcal{M}_{n,d}(\\mathbb{R})$, donde $n$ el largo de la secuencia y $d$ es la dimensión de cada elemento de la secuencia. Al interpretar una imagen como secuencia (para así usar self-attention) se tendrá que $n=\\text{alto}\\cdot\\text{ancho}$ y $d=\\text{número de canales}$.\n",
    "\n",
    "En un cabezal de self-attention convencional, la entrada $X\\in\\mathcal{M}_{n,d}(\\mathbb{R})$ en proyectada 3 veces para formar 3 matrices $Q,\\,K,\\,V\\in\\mathcal{M}_{n,d_k}(\\mathbb{R})$ (i.e., para formar cada una de estas matrices, los elementos de la secuencia (filas de $X$) son pasados por una capa lineal). Luego, cada cabezal de atención $H$ queda definido mediante \n",
    "\n",
    "$$\n",
    "H = \\text{softmax}_{\\text{por filas}}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Finalmente, un módulo de self-attention consiste en varios cabezales concatenados y luego proyectados para reducir la dimensión de la concatenación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - n_channels: cantidad de canales en la entrada.\n",
    "            - n_heads: cantidad de cabezales.\n",
    "            - d_k: dimensión de las matrices Q, K, V y de la salida de cabezal.\n",
    "            - n_groups: cantidad de grupos para group normalization.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "\n",
    "        # Proyecciones para las matrices Q, K, V de todos los cabezales:\n",
    "        self.qkv_projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        \n",
    "        # Proyección para la concatenación de cabezales:\n",
    "        self.final_projection = nn.Linear(d_k * n_heads, n_channels)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, n_channels, height, width]: batch de entrada.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, n_channels, height, width]: salida del módulo de self-attention.\n",
    "        '''\n",
    "\n",
    "        batch_size, n_channels, height, width = input.shape\n",
    "        seq_length = height * width\n",
    "\n",
    "        # Transformar el input en un batch de secuencias ([batch_size, seq_length, n_channels]):\n",
    "        input = input.view(batch_size, n_channels, seq_length).permute(0, 2, 1)\n",
    "\n",
    "        # Matrices Q, K, V (de todos los cabezales):\n",
    "        qkv = self.qkv_projection(input)  # [batch_size, seq_length, n_heads * d_k * 3].\n",
    "        qkv = qkv.view(batch_size, seq_length, self.n_heads, 3 * self.d_k)\n",
    "        q, k, v = qkv.split(self.d_k, dim=-1)  # cada tensor de tamaño [batch_size, seq_length, n_heads, d_k].\n",
    "        \n",
    "        # Producto externo ([QK^T]_ij = <fila_i(Q), fila_j(K)>) y softmax:\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k)  # [batch_size, seq_length, seq_length, h_heads].\n",
    "        attn = torch.softmax(attn / (self.d_k ** 0.5), dim=2)\n",
    "\n",
    "        # Producto atención-values:\n",
    "        output = torch.einsum('bijh,bjhd->bihd', attn, v)  # [batch_size, seq_length, h_heads, d_k].\n",
    "        \n",
    "        # Reshape en forma de cabezales concatenados:\n",
    "        output = output.permute(0, 1, 3, 2)  # [batch_size, seq_length, d_k, h_heads].\n",
    "        output = output.reshape(batch_size, seq_length, self.d_k * self.n_heads)\n",
    "\n",
    "        # Proyección final y conexión residual:\n",
    "        output = self.final_projection(output)  # [batch_size, seq_length, n_channels].\n",
    "        output += input\n",
    "\n",
    "        # Recuperar tamaño como batch de imágenes:\n",
    "        output = output.permute(0, 2, 1)  # [batch_size, n_channels, seq_length].\n",
    "        output = output.view(batch_size, n_channels, height, width)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "n_channels, n_heads, d_k = 64, 2, 8\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "attention_block = AttentionBlock(n_channels, n_heads, d_k)\n",
    "input = torch.randn([batch_size, n_channels, height, width])\n",
    "\n",
    "assert attention_block(input).shape == torch.Size([batch_size, n_channels, height, width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloques principales de la U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se crearán clases simples para los bloques principales que se observan en la arquitectura U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloques `ResidualBlock` + `AttentionBlock`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bloques descendentes y ascendentes de la U buscan cambiar la cantidad de canales del input, sin cambiar la resolución de las imágenes. Ambos tipos de bloque son de la forma `bloque residual -> self-attention`. En la inicialización de estos bloques se debe indicar si usar self-attention ya que no todos los bloques de la U-Net lo utilizan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - in_channels: cantidad de canales en la entrada.\n",
    "            - out_channels: cantidad de canales en la salida.\n",
    "            - time_channels: cantidad de canales para el embedding temporal.\n",
    "            - has_attn: indica si el módulo aplicará atención luego del bloque residual.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_block = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attention_block = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attention_block = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor, time_embedding: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, in_channels, height, width]: batch de entrada.\n",
    "            - time_embedding[batch_size, time_channels]: batch de embedding temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, out_channels, height, width]: batch de salida.\n",
    "        '''\n",
    "        \n",
    "        output = self.residual_block(input, time_embedding)\n",
    "        output = self.attention_block(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "in_channels, out_channels, time_channels, has_attn = 96, 64, 5, True\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "main_block = MainBlock(in_channels, out_channels, time_channels, has_attn)\n",
    "input = torch.randn([batch_size, in_channels, height, width])\n",
    "time_embedding = torch.randn([batch_size, time_channels])\n",
    "output = main_block(input, time_embedding)\n",
    "\n",
    "assert output.shape == torch.Size([batch_size, out_channels, height, width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bloque intermedio de la U es de la forma `bloque residual -> self-attention -> bloque residual`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - n_channels: cantidad de canales en la entrada y en la salida.\n",
    "            - time_channels: cantidad de canales para el embedding temporal.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_block1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attention_block = AttentionBlock(n_channels)\n",
    "        self.residual_block2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor, time_embedding: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, n_channels, height, width]: batch de entrada.\n",
    "            - time_embedding[batch_size, time_channels]: batch de embedding temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, n_channels, height, width]: batch de salida.\n",
    "        '''\n",
    "\n",
    "        output = self.residual_block1(input, time_embedding)\n",
    "        output = self.attention_block(output)\n",
    "        output = self.residual_block2(input, time_embedding)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "n_channels, time_channels = 32, 5\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "middle_block = MiddleBlock(n_channels, time_channels)\n",
    "input = torch.randn([batch_size, n_channels, height, width])\n",
    "time_embedding = torch.randn([batch_size, time_channels])\n",
    "output = middle_block(input, time_embedding)\n",
    "\n",
    "assert output.shape == torch.Size([batch_size, n_channels, height, width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloques de downsampling y upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos bloques se utilizarán para cambiar la resolución de las imágenes. El bloque `DecreaseResolution` disminuirá el alto y ancho por 2, mientras que el bloque `IncreaseResolution` amplificará por 2. En algunas variantes de esta arquitectura se utiliza `MaxPool2d` para el downsampling (en vez de convolución) y `Upsample` para el upsampling (en vez de deconvolución). Estos métodos aceleran el procesamiento y simplifican el modelo ya que no tienen parámetros aprendibles.\n",
    "\n",
    "Si bien estos bloques consisten únicamente en una convolución (o convolución transpuesta) y podrían introducirse directamente en la clase principal de la U-Net, aquí se modifica el forward para que también pueda recibir el embedding temporal (el cual no será usado). Esto permitirá poder trabajar los bloques `DecreaseResolution` e `IncreaseResolution` igual que los bloques `MainBlock` y `MiddleBlock` en el forward de la U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecreaseResolution(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        '''\n",
    "        Parameters:\n",
    "        - n_channels: cantidad de canales en la entrada y en la salida,\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, 3, stride=2, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_embedding: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, n_channels, height, width]: batch de entrada.\n",
    "            - time_embedding[batch_size, time_channels]: batch de embedding temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, n_channels, height / 2, width / 2]: batch de salida.\n",
    "        '''\n",
    "        \n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "n_channels, time_channels = 32, 5\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "decrease_resolution = DecreaseResolution(n_channels)\n",
    "input = torch.randn([batch_size, n_channels, height, width])\n",
    "time_embedding = torch.randn([batch_size, time_channels])\n",
    "output = decrease_resolution(input, time_embedding)\n",
    "\n",
    "assert output.shape == torch.Size([batch_size, n_channels, height // 2, width // 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo de upsampling es igual al anterior pero usando una convolución transpuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncreaseResolution(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        '''\n",
    "        Parameters:\n",
    "        - n_channels: cantidad de canales en la entrada y en la salida,\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, 4, stride=2, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_embedding: torch.Tensor):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, n_channels, height, width]: batch de entrada.\n",
    "            - time_embedding[batch_size, time_channels]: batch de embedding temporales.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, n_channels, height * 2, width * 2]: batch de salida.\n",
    "        '''\n",
    "        \n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "n_channels, time_channels = 32, 5\n",
    "batch_size, height, width = 128, 32, 32\n",
    "\n",
    "increase_resolution = IncreaseResolution(n_channels)\n",
    "input = torch.randn([batch_size, n_channels, height, width])\n",
    "time_embedding = torch.randn([batch_size, time_channels])\n",
    "output = increase_resolution(input, time_embedding)\n",
    "\n",
    "assert output.shape == torch.Size([batch_size, n_channels, height * 2, width * 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, inital_channels: int = 64, channel_factors: tuple = (1, 2, 2, 2),\n",
    "                 has_attn: tuple = (False, False, False, False), n_blocks: int = 2):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - image_channels: cantidad de canales de las imágenes.\n",
    "            - inital_channels: cantidad de canales luego de la proyección inicial.\n",
    "            - channel_factors: factores de reducción/amplificación de la cantidad de canales.\n",
    "            - has_attn: indica si se usará o no atención en los bloques residuales de cada factor.\n",
    "            - n_blocks: cantidad de bloques principales en cada factor.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        time_channels = inital_channels * 4\n",
    "        self.n_blocks = n_blocks\n",
    "        self.time_embedding = TimeEmbedding(time_channels)\n",
    "\n",
    "        # Proyección inicial:\n",
    "        self.image_projection = nn.Conv2d(image_channels, inital_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Parte descendente:\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        in_channels = inital_channels\n",
    "        for i, factor in enumerate(channel_factors):\n",
    "\n",
    "            out_channels = in_channels * factor\n",
    "            for _ in range(n_blocks):\n",
    "                self.down_blocks.append(MainBlock(in_channels, out_channels, time_channels, has_attn[i]))\n",
    "                in_channels = out_channels\n",
    "\n",
    "            self.down_blocks.append(DecreaseResolution(out_channels))\n",
    "\n",
    "        # Parte media:\n",
    "        self.middle = MiddleBlock(out_channels, time_channels)\n",
    "\n",
    "        # Parte ascendente:\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "\n",
    "        for i, factor in enumerate(reversed(channel_factors)):\n",
    "\n",
    "            self.up_blocks.append(IncreaseResolution(in_channels))\n",
    "\n",
    "            for n in range(n_blocks):\n",
    "                if n == 0:\n",
    "                    in_channels *= 2  # primer bloque recibe skip conection.\n",
    "                elif n == n_blocks - 1:\n",
    "                    out_channels //= factor  # último bloque reduce resolución.\n",
    "                \n",
    "                self.up_blocks.append(MainBlock(in_channels, out_channels, time_channels, has_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            \n",
    "        # Capas finales:\n",
    "        self.last_normalization = nn.GroupNorm(8, inital_channels)\n",
    "        self.last_activation = nn.SiLU()\n",
    "        self.projection_to_image = nn.Conv2d(in_channels, image_channels, 3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, input, times):\n",
    "        '''\n",
    "        Parameters:\n",
    "            - input[batch_size, in_channels, height, width]: batch de imágenes.\n",
    "            - times[batch_size]: tiempos asociados a cada imagen.\n",
    "        \n",
    "        Returns:\n",
    "            - output[batch_size, in_channels, height, width]: salida de la U-Net.\n",
    "        '''\n",
    "\n",
    "        time_embedding = self.time_embedding(times)\n",
    "        input = self.image_projection(input)\n",
    "\n",
    "        # Encoding:\n",
    "        down_outputs = [input]\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            input = block(input, time_embedding)\n",
    "            if i % (self.n_blocks + 1) == 0:\n",
    "                down_outputs.append(input)\n",
    "\n",
    "        # Centro:\n",
    "        input = self.middle(input, time_embedding)\n",
    "\n",
    "        # Decoding:\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            if i % (self.n_blocks + 1) == 2:\n",
    "                down_connection = down_outputs.pop()\n",
    "                input = torch.cat((input, down_connection), dim=1)\n",
    "            input = block(input, time_embedding)\n",
    "\n",
    "        input = self.projection_to_image(self.last_activation(self.last_normalization(input)))\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, in_channels, height, width = 2, 3, 512, 512\n",
    "\n",
    "unet = UNet()\n",
    "input = torch.randn([batch_size, in_channels, height, width])\n",
    "times = torch.randn([batch_size])\n",
    "\n",
    "assert unet(input, times).shape == torch.Size([batch_size, in_channels, height, width])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
